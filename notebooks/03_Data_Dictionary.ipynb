{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6911ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📘 Complete Feature Dictionary (English)</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Group 1: Bibliographic Features (The \"What\" and \"When\")\n",
    "1.  **doi**:\n",
    "    *   **Meaning:** The unique identifier of the article.\n",
    "    *   **Importance:** Not a feature for the model, but the primary key to join all our data.\n",
    "2.  **publication_year**:\n",
    "    *   **Meaning:** The year the article was published.\n",
    "    *   **Importance:** Provides temporal context. Publishing and retraction practices change over time.\n",
    "    *   **Hypothesis:** The retraction rate may have increased in recent years due to better detection tools or the pressure to publish.\n",
    "3.  **article_type**:\n",
    "    *   **Meaning:** The type of publication (e.g., 'article', 'conference', 'review').\n",
    "    *   **Importance:** Essential for comparing \"apples to apples.\" The standards of rigor are different for each type.\n",
    "4.  **is_open_access**:\n",
    "    *   **Meaning:** A boolean (1/0) indicating if the article is open access.\n",
    "    *   **Importance:** Represents the journal's business model.\n",
    "    *   **Hypothesis:** Predatory journals often use a \"pay-to-publish\" open-access model, which could correlate with a higher retraction rate. That is, is there a correlation between the access model (and its potential lack of rigor in some cases) and retractions?\n",
    "\n",
    "### Group 2: Authorship Features (The \"Who\")\n",
    "1.  **author_count**:\n",
    "    *   **Meaning:** Total number of authors.\n",
    "    *   **Importance:** Measures the size of the research team.\n",
    "    *   **Hypothesis:** Anomalous numbers (very high or very low) could be a sign of \"paper mills\" or fraudulent authorships.\n",
    "2.  **institution_count**:\n",
    "    *   **Meaning:** Number of unique research institutions involved.\n",
    "    *   **Importance:** Measures the diversity of the collaboration.\n",
    "3.  **country_count**:\n",
    "    *   **Meaning:** Number of unique countries involved.\n",
    "    *   **Importance:** Similar to the previous one, but at a geopolitical level.\n",
    "4.  **first_author_country**:\n",
    "    *   **Meaning:** The country of the first author.\n",
    "    *   **Importance:** A strong contextual signal, as the first author usually leads the research.\n",
    "    *   **Hypothesis:** Certain national scientific policies could correlate with higher retraction rates.\n",
    "5.  **is_international_collaboration**:\n",
    "    *   **Meaning:** A boolean (1/0) if more than one country is involved.\n",
    "    *   **Importance:** A simple and powerful feature that summarizes international collaboration.\n",
    "\n",
    "### Group 3: Publication Venue Features (The \"Where\")\n",
    "1.  **journal_name**:\n",
    "    *   **Meaning:** Name of the journal or conference.\n",
    "    *   **Importance:** Useful for exploratory analysis and for understanding the model's results.\n",
    "2.  **publisher**:\n",
    "    *   **Meaning:** The publishing organization of the journal (e.g., 'Elsevier', 'Hindawi').\n",
    "    *   **Importance:** A very powerful feature. As we saw, a few publishers account for a large number of retractions.\n",
    "    *   **Hypothesis:** The business model and review standards of certain publishers are key predictors of retraction risk.\n",
    "3.  **is_publisher_missing**:\n",
    "    *   **Meaning:** A boolean flag (1/0) that is 1 (True) if a publisher could not be identified for the article, and 0 (False) if it was found.\n",
    "    *   **Importance:** Acts as a proxy for the \"legitimacy\" of the source. Serious, well-indexed publications almost always have a clear publisher. Its absence is a red flag.\n",
    "    *   **Hypothesis:** The lack of an identifiable publisher (a value of 1) correlates with a higher risk of retraction, as it may indicate that the article comes from a less-established source, a dubious conference, or a predatory journal.\n",
    "4.  **source_id**:\n",
    "    *   **Meaning:** The unique OpenAlex ID for the journal/source.\n",
    "    *   **Importance:** Not for the current model, but it's an investment for the future. We save it to be able to enrich the dataset later if we find a reliable source of journal metrics.\n",
    "\n",
    "### Group 4: Content Features (The \"How\" - NLP)\n",
    "1.  **title** and **abstract**:\n",
    "    *   **Meaning:** The text of the title and abstract (cleaned of retraction notices).\n",
    "    *   **Importance:** They are the raw material for creating more complex NLP features (like embeddings).\n",
    "2.  **is_abstract_missing**:\n",
    "    *   **Meaning:** A boolean flag (1/0) that is 1 (True) if the article does not have a real, clean abstract, and 0 (False) if it does.\n",
    "    *   **Importance:** Measures the \"completeness\" of the article's record. A serious research paper almost always includes an abstract.\n",
    "    *   **Hypothesis:** The absence of an abstract is an anomalous characteristic and could be associated with lower-quality works, conference summaries, or fraudulent articles that avoid giving a clear summary of their methods and results.\n",
    "3.  **title_length** and **abstract_length**:\n",
    "    *   **Meaning:** The number of characters in the title and in the actual abstract.\n",
    "    *   **Importance:** Simple measures of text structure.\n",
    "    *   **Hypothesis:** Fraudulent or low-quality works might have abnormally short or long abstracts or titles.\n",
    "4.  **n_concepts**:\n",
    "    *   **Meaning:** The number of topics or concepts that OpenAlex associates with the article.\n",
    "    *   **Importance:** Measures thematic breadth.\n",
    "    *   **Hypothesis:** \"Topic stuffing.\" An article that claims to cover too many things could be a sign of a lack of focus or an attempt to deceive.\n",
    "5.  **top_concept_level**:\n",
    "    *   **Meaning:** The level of specificity of the main concept (0=very general, 5=very specific).\n",
    "    *   **Importance:** Measures the depth of the topic.\n",
    "    *   **Hypothesis:** It might be easier to publish fraudulent works on very general and vague topics (low level) than on very specific and technical topics (high level).\n",
    "\n",
    "### Group 5: Impact Features (The \"So What\")\n",
    "1.  **n_references**:\n",
    "    *   **Meaning:** The number of articles this work cites in its bibliography.\n",
    "    *   **Importance:** A proxy for the background research work.\n",
    "    *   **Hypothesis:** Anomalous numbers (very few or very many) could be a sign of a careless or fraudulent work.\n",
    "2.  **citations_in_first_2_years**:\n",
    "    *   **Meaning:** An integer representing the total citations an article received during its first two years of life (the year of publication and the following one).\n",
    "    *   **Importance:** It is a measure of early impact that is temporally controlled, which prevents data leakage by not counting citations that occurred after an article was retracted. It is methodologically much more robust than the total citation count.\n",
    "    *   **Hypothesis:** High-quality (and low-risk) articles tend to accumulate citations in a healthy and steady manner from the beginning. Problematic articles might show anomalous citation patterns: either zero impact (no one cites them) or, in cases of organized fraud, a suspicious peak of self-citations in the initial phase.\n",
    "\n",
    "### Features We Don't Have Yet\n",
    "1.  **journal_h_index**:\n",
    "    *   **Meaning:** A metric that combines the productivity and impact of a journal. A high number indicates greater prestige.\n",
    "    *   **Importance (Theoretical):** It was our best feature to numerically measure the \"rigor\" of a journal.\n",
    "    *   **Hypothesis (Strong but unverified):** A low H-Index correlates strongly with a higher risk of retraction.\n",
    "    *   **Current Status:** Discarded for now due to the difficulty of obtaining it reliably and scalably, but we are keeping it on our \"radar\" thanks to saving the `source_id`.\n",
    "2.  **avg_author_works_count**:\n",
    "    *   **Meaning:** The average of the total number of publications for each author of the article. It measured the average \"experience\" or \"productivity\" of the team.\n",
    "    *   **Importance (Theoretical):** It was a numerical proxy for the history and reputation of the author team.\n",
    "    *   **Hypothesis:** Retracted articles might be written by teams with a significantly lower average of previous publications (novice or fraudulent authors).\n",
    "    *   **Reason for Discarding:** Computational Infeasibility. The OpenAlex API does not provide the author's `works_count` in the article query. Obtaining it would require an extra API call for each author of each article (~250,000+ additional calls), which is too costly and slow for the value it provides.\n",
    "\n",
    "### Features We Initially Planned to Use But Ultimately Discarded\n",
    "1.  **is_abstract_retraction_notice**:\n",
    "    *   **Meaning:** A boolean (1/0) indicating if the abstract field contained a retraction notice.\n",
    "    *   **Importance:** It is a meta-feature about data quality.\n",
    "    *   **Hypothesis:** The fact that the publisher replaces the abstract is a strong signal, although it occurs after the retraction (which is why we discarded it). For the model, it could capture subtle patterns about how different publishers handle retractions.\n",
    "\n",
    "The next two were replaced by `citations_in_first_2_years` to avoid data leakage.\n",
    "\n",
    "2.  **citation_count_total**:\n",
    "    *   **Meaning:** The total number of times the article has been cited.\n",
    "    *   **Importance:** A classic measure of impact.\n",
    "3.  **citations_per_year**:\n",
    "    *   **Meaning:** Total citations divided by the \"age\" of the article.\n",
    "    *   **Importance:** A much fairer, normalized measure of impact. It measures the \"speed\" at which an article accumulates citations.\n",
    "    *   **Hypothesis:** Fraudulent articles might have an initial peak of citations that then drops off quickly, or simply a very low citation speed.\n",
    "\n",
    "---\n",
    "\n",
    "### From the `df_retraction` after cleaning, we have these 21 columns and 56,046 rows:\n",
    "\n",
    "```\n",
    "Index(['record_id', 'title', 'subject', 'institution', 'journal', 'publisher',\n",
    "'country', 'author', 'urls', 'article_type', 'retraction_date',\n",
    "'retraction_doi', 'retraction_pub_med_id', 'original_paper_date',\n",
    "'original_paper_doi', 'original_paper_pub_med_id', 'retraction_nature',\n",
    "'reason', 'paywalled', 'notes', 'year'],\n",
    "dtype='object')\n",
    "```\n",
    "\n",
    "### Columns We DID Use (Directly or Indirectly)\n",
    "*   **`original_paper_doi`**:\n",
    "    *   **Usage:** Fundamental. It's our \"master key,\" the shopping list of DOIs we use to query the OpenAlex API and enrich the data.\n",
    "*   **`title`, `subject`, `institution`, `journal`, `publisher`, `country`, `author`, `article_type`, `original_paper_date (year)`**:\n",
    "    *   **Usage:** Inspiration and Validation. We don't use their data directly for the model (because we prefer the clean data from the API), but they were crucial for:\n",
    "        1.  **Formulating hypotheses:** Analyzing these columns helped us decide which features were important (publisher, country, etc.).\n",
    "        2.  **Validating the extraction:** We compared the data from these columns with what we get from the API to ensure our extractor works correctly.\n",
    "*   **`paywalled`**:\n",
    "    *   **Usage:** Inspiration. It led us to create the `is_open_access` feature from the API, which is a more reliable and standardized version.\n",
    "\n",
    "### Columns We WILL Add to the Final Dataset\n",
    "*   **`retraction_nature`**:\n",
    "    *   **Usage:** Target Definition. We use it to filter and keep only the cases that are \"Retraction\" or \"Expression of concern,\" thus defining what a \"bad\" event we want to predict is.\n",
    "*   **`reason`**:\n",
    "    *   **Usage:** The Crown Jewel. Although not a direct predictive feature, it is crucial for post-model analysis. Once the model identifies a paper as \"high risk,\" we can look at this column in our training data to understand why similar articles were retracted (e.g., \"Fake Peer Review,\" \"Paper Mill\"). It inspires feature engineering and enriches the project's narrative.\n",
    "\n",
    "### Columns We DID NOT Use (and why)\n",
    "*   **`record_id`**:\n",
    "    *   **Reason:** It's an internal identifier for Retraction Watch. It has no predictive value and is not a useful key for joining with other databases.\n",
    "*   **`urls`**:\n",
    "    *   **Reason:** Too heterogeneous and unstructured. The DOI URL already gives us the canonical link to the article.\n",
    "*   **`retraction_date`**:\n",
    "    *   **Reason:** Data Leakage. This is the date the event occurred. We cannot use information from the future to predict the past. Our model must predict the risk of retraction at the time of publication, when this date does not yet exist.\n",
    "*   **`retraction_doi`, `retraction_pub_med_id`**:\n",
    "    *   **Reason:** They are the identifiers of the retraction notice, not the original paper. They are not relevant for analyzing the characteristics of the article itself.\n",
    "*   **`original_paper_pub_med_id`**:\n",
    "    *   **Reason:** Redundant. The `original_paper_doi` is the universal identifier we have chosen as our primary key.\n",
    "*   **`notes`**:\n",
    "    *   **Reason:** Unstructured free text. It contains valuable information for manual investigation but is very difficult to process into a quantitative feature useful for the model. We discarded it for pragmatic reasons.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3c7f7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📘 Diccionario completo de features (Español)</strong></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Grupo 1: Características Bibliográficas (El \"Qué\" y \"Cuándo\")\n",
    "1. doi:\n",
    "- Significado: El identificador único del artículo.\n",
    "- Importancia: No es una feature para el modelo, sino la llave primaria para unir todos nuestros datos.\n",
    "2. publication_year:\n",
    "- Significado: Año en que se publicó el artículo.\n",
    "- Importancia: Aporta contexto temporal. Las prácticas de publicación y retractación cambian con el tiempo.\n",
    "- Hipótesis: La tasa de retractaciones puede haber aumentado en los últimos años debido a mejores herramientas de detección o a la presión por publicar.\n",
    "3. article_type:\n",
    "- Significado: El tipo de publicación (ej: 'article', 'conference', 'review').\n",
    "- Importancia: Esencial para comparar \"manzanas con manzanas\". Los estándares de rigor son diferentes para cada tipo.\n",
    "4. is_open_access:\n",
    "- Significado: Booleano (1/0) que indica si el artículo es de acceso abierto.\n",
    "- Importancia: Representa el modelo de negocio de la revista.\n",
    "- Hipótesis: Las revistas depredadoras a menudo usan un modelo \"paga por publicar\" de acceso abierto, lo que podría correlacionarse con una mayor tasa de retractación. Es decir, ¿existe una correlación entre el modelo de acceso (y su posible falta de rigor en algunos casos) y las retractaciones?\n",
    "### Grupo 2: Características de Autoría (El \"Quién\")\n",
    "1. author_count:\n",
    "- Significado: Número total de autores.\n",
    "- Importancia: Mide el tamaño del equipo de investigación.\n",
    "- Hipótesis: Números anómalos (muy altos o muy bajos) podrían ser una señal de \"paper mills\" o autorías fraudulentas.\n",
    "2. institution_count:\n",
    "- Significado: Número de instituciones de investigación únicas involucradas.\n",
    "- Importancia: Mide la diversidad de la colaboración.\n",
    "3. country_count:\n",
    "- Significado: Número de países únicos involucrados.\n",
    "- Importancia: Similar al anterior, pero a nivel geopolítico.\n",
    "4. first_author_country:\n",
    "- Significado: El país del primer autor.\n",
    "- Importancia: Fuerte señal contextual, ya que el primer autor suele liderar la investigación.\n",
    "- Hipótesis: Ciertas políticas científicas nacionales podrían correlacionarse con mayores tasas de retractación.\n",
    "5. is_international_collaboration:\n",
    "- Significado: Booleano (1/0) si hay más de un país involucrado.\n",
    "- Importancia: Feature simple y potente que resume la colaboración internacional.\n",
    "### Grupo 3: Características del Lugar de Publicación (El \"Dónde\")\n",
    "1. journal_name:\n",
    ". Significado: Nombre de la revista o conferencia.\n",
    "- Importancia: Útil para el análisis exploratorio y para entender los resultados del modelo.\n",
    "2. publisher:\n",
    "- Significado: La editorial que publica la revista (ej: 'Elsevier', 'Hindawi').\n",
    "- Importancia: Feature potentísima. Como vimos, unas pocas editoriales concentran un gran número de retractaciones.\n",
    "- Hipótesis: El modelo de negocio y los estándares de revisión de ciertas editoriales son un predictor clave del riesgo de retractación.\n",
    "3. is_publisher_missing:\n",
    "- Significado: Un flag booleano (1/0) que es 1 (Verdadero) si no se pudo identificar un editor para el artículo, y 0 (Falso) si sí se encontró.\n",
    "- Importancia: Actúa como un proxy de la \"legitimidad\" de la fuente. Las publicaciones serias y bien indexadas casi siempre tienen un editor claro. Su ausencia es una señal de alerta.\n",
    "- Hipótesis: La falta de un editor identificable (un valor de 1) se correlaciona con un mayor riesgo de retractación, ya que puede indicar que el artículo proviene de una fuente poco establecida, una conferencia dudosa o una revista depredadora.\n",
    "3. source_id:\n",
    "- Significado: El ID único de OpenAlex para la revista/fuente.\n",
    "- Importancia: No es para el modelo actual, pero es una inversión de futuro. La guardamos para poder enriquecer el dataset más adelante si encontramos una fuente fiable de métricas de revistas.\n",
    "### Grupo 4: Características de Contenido (El \"Cómo\" - NLP)\n",
    "1. title y abstract:\n",
    "- Significado: El texto del título y del resumen (limpio de avisos de retractación).\n",
    "- Importancia: Son la materia prima para crear features de NLP más complejas (como embeddings).\n",
    "2. is_abstract_missing:\n",
    "- Significado: Un flag booleano (1/0) que es 1 (Verdadero) si el artículo no tiene un resumen (abstract) real y limpio, y 0 (Falso) si lo tiene.\n",
    "- Importancia: Mide la \"completitud\" del registro del artículo. Un trabajo de investigación serio casi siempre incluye un abstract.\n",
    "- Hipótesis: La ausencia de un abstract es una característica anómala y podría estar asociada con trabajos de menor calidad, resúmenes de conferencias, o artículos fraudulentos que evitan dar un resumen claro de sus métodos y resultados.\n",
    "3. title_length y abstract_length:\n",
    "- Significado: El número de caracteres en el título y en el abstract real.\n",
    "- Importancia: Medidas simples de la estructura del texto.\n",
    "- Hipótesis: Trabajos fraudulentos o de baja calidad podrían tener abstracts o títulos anormalmente cortos o largos.\n",
    "4. n_concepts:\n",
    "- Significado: Número de temas o conceptos que OpenAlex asocia al artículo.\n",
    "- Importancia: Mide la amplitud temática.\n",
    "- Hipótesis: \"Topic stuffing\". Un artículo que dice tratar sobre demasiadas cosas podría ser una señal de falta de enfoque o de intento de engaño.\n",
    "5. top_concept_level:\n",
    "- Significado: El nivel de especificidad del concepto principal (0=muy general, 5=muy específico).\n",
    "- Importancia: Mide la profundidad del tema.\n",
    "- Hipótesis: Podría ser más fácil publicar trabajos fraudulentos sobre temas muy generales y vagos (nivel bajo) que sobre temas muy específicos y técnicos (nivel alto).\n",
    "### Grupo 5: Características de Impacto (El \"Y Qué\")\n",
    "1. n_references:\n",
    "- Significado: El número de artículos que este trabajo cita en su bibliografía.\n",
    "- Importancia: Proxy del trabajo de investigación de fondo.\n",
    "- Hipótesis: Números anómalos (muy pocos o muchísimos) podrían ser una señal de un trabajo descuidado o fraudulento.\n",
    "2. citations_in_first_2_years:\n",
    "- Significado: Un número entero que representa el total de citas que un artículo recibió durante sus primeros dos años de vida (el año de publicación y el siguiente).\n",
    "- Importancia: Es una medida de impacto temprano que está controlada temporalmente, lo que evita la fuga de datos (data leakage) al no contar citas que ocurrieron después de que un artículo fuera retractado. Es metodológicamente mucho más robusta que el conteo total de citas.\n",
    "- Hipótesis: Los artículos de alta calidad (y bajo riesgo) tienden a acumular citas de forma saludable y constante desde el principio. Los artículos problemáticos podrían mostrar patrones de citación anómalos: o bien un impacto nulo (nadie los cita) o, en casos de fraude organizado, un pico de auto-citaciones sospechoso en la fase inicial.\n",
    "\n",
    "### Features que todavía no tenemos\n",
    "1. journal_h_index:\n",
    "- Significado: Métrica que combina la productividad y el impacto de una revista. Un número alto indica mayor prestigio.\n",
    "- Importancia (Teórica): Era nuestra mejor feature para medir numéricamente el \"rigor\" de una revista.\n",
    "- Hipótesis (Fuerte pero no verificada): Un H-Index bajo se correlaciona fuertemente con un mayor riesgo de retractación.\n",
    "- Estado Actual: Descartada por ahora debido a la dificultad de obtenerla de forma fiable y escalable, pero la conservamos en nuestro \"radar\" gracias a que guardamos el source_id.\n",
    "2. avg_author_works_count:\n",
    "- Significado: La media del número total de publicaciones de cada autor del artículo. Medía la \"experiencia\" o \"productividad\" promedio del equipo.\n",
    "- Importancia (Teórica): Era un proxy numérico del historial y la reputación del equipo de autores.\n",
    "- Hipótesis: Los artículos retractados podrían ser escritos por equipos con una media de publicaciones previas significativamente menor (autores novatos o fraudulentos).\n",
    "- Razón del Descarte: Inviabilidad Computacional. La API de OpenAlex no proporciona el works_count del autor en la consulta del artículo. Obtenerlo requeriría una llamada extra a la API por cada autor de cada artículo (~250,000+ llamadas adicionales), lo cual es demasiado costoso y lento para el valor que aporta.\n",
    "\n",
    "### Features que inicilamente ibamos a usar pero han acabado descartadas\n",
    "1. is_abstract_retraction_notice:\n",
    "- Significado: Booleano (1/0) que indica si el campo del abstract contenía un aviso de retractación.\n",
    "- Importancia: Es una feature meta sobre la calidad del dato.\n",
    "- Hipótesis: El hecho de que el editor reemplace el abstract es una señal fuerte, aunque ocurre después de la retractación (por eso la descartamos). Para el modelo, podría capturar patrones sutiles sobre cómo diferentes editoriales manejan las retractaciones.\n",
    "\n",
    "Las siguientes dos, se reemplazaron por citatioins_in_first_2_years para evitar fuga de datos\n",
    "\n",
    "2. citation_count_total:\n",
    "- Significado: El número total de veces que el artículo ha sido citado.\n",
    "- Importancia: Medida clásica de impacto.\n",
    "3. citations_per_year:\n",
    "- Significado: Citas totales divididas por la \"edad\" del artículo.\n",
    "- Importancia: Una medida de impacto normalizada y mucho más justa. Mide la \"velocidad\" a la que un artículo acumula citas.\n",
    "- Hipótesis: Los artículos fraudulentos podrían tener un pico inicial de citas que luego decae rápidamente, o simplemente una velocidad de citación muy baja.\n",
    "\n",
    "---\n",
    "\n",
    "### Del df_retraction después de limpiarlo, tenemos estas 21 columnas y 56046 filas:\n",
    "\n",
    "Index(['record_id', 'title', 'subject', 'institution', 'journal', 'publisher',\n",
    "'country', 'author', 'urls', 'article_type', 'retraction_date',\n",
    "'retraction_doi', 'retraction_pub_med_id', 'original_paper_date',\n",
    "'original_paper_doi', 'original_paper_pub_med_id', 'retraction_nature',\n",
    "'reason', 'paywalled', 'notes', 'year'],\n",
    "dtype='object')\n",
    "\n",
    "### Columnas que SÍ usamos (Directa o Indirectamente)\n",
    "- original_paper_doi:\n",
    "    - Uso: Fundamental. Es nuestra \"llave maestra\", la lista de la compra de DOIs que usamos para consultar la API de OpenAlex y enriquecer los datos.\n",
    "- title, subject, institution, journal, publisher, country, author, article_type, original_paper_date (year):\n",
    "    - Uso: Inspiración y Validación. No usamos sus datos directamente para el modelo (porque preferimos los datos limpios de la API), pero han sido cruciales para:\n",
    "        1. Formular hipótesis: Analizar estas columnas nos ayudó a decidir qué features eran importantes (publisher, country, etc.).\n",
    "        2. Validar la extracción: Comparamos los datos de estas columnas con los que obtenemos de la API para asegurarnos de que nuestro extractor funciona correctamente.\n",
    "- paywalled:\n",
    "    - Uso: Inspiración. Nos llevó a crear la feature is_open_access a partir de la API, que es una versión más fiable y estandarizada.\n",
    "### Columnas que SÍ añadiremos al Dataset Final\n",
    "- retraction_nature:\n",
    "    - Uso: Definición del Target. La usamos para filtrar y quedarnos solo con los casos que son \"Retraction\" o \"Expression of concern\", definiendo así qué es un evento \"malo\" que queremos predecir.\n",
    "- reason:\n",
    "    - Uso: La Joya de la Corona. Aunque no es una feature predictiva directa, es crucial para el análisis post-modelo. Una vez que el modelo identifica un paper como de \"alto riesgo\", podemos mirar esta columna en nuestros datos de entrenamiento para entender por qué artículos similares fueron retractados (ej. \"Fake Peer Review\", \"Paper Mill\"). Inspira la ingeniería de features y enriquece la narrativa del proyecto.\n",
    "### Columnas que NO usamos (y por qué)\n",
    "- record_id:\n",
    "    - Motivo: Es un identificador interno de Retraction Watch. No tiene valor predictivo ni es una clave útil para unir con otras bases de datos.\n",
    "- urls:\n",
    "    - Motivo: Demasiado heterogéneo y poco estructurado. La URL del DOI ya nos da el enlace canónico al artículo.\n",
    "- retraction_date:\n",
    "    - Motivo: Fuga de Datos (Data Leakage). Esta es la fecha en que el evento ocurrió. No podemos usar información del futuro para predecir el pasado. Nuestro modelo debe predecir el riesgo de retractación en el momento de la publicación, cuando esta fecha aún no existe.\n",
    "- retraction_doi, retraction_pub_med_id:\n",
    "    - Motivo: Son los identificadores del aviso de retractación, no del paper original. No son relevantes para analizar las características del artículo en sí.\n",
    "- original_paper_pub_med_id:\n",
    "    - Motivo: Redundante. El original_paper_doi es el identificador universal que hemos elegido como nuestra clave principal.\n",
    "- notes:\n",
    "    - Motivo: Texto libre no estructurado. Contiene información valiosa para una investigación manual, pero es muy difícil de procesar para convertirlo en una feature cuantitativa útil para el modelo. Lo descartamos por pragmatismo.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
